{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_dir = 'To be updated'\n",
    "time_dir = 'To be updated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_discount(df, sku = None, year_col='Year'):\n",
    "    out_df = df.copy()\n",
    "    price_col = f'{sku}_Price' if sku is not None else 'Price'\n",
    "    # Calculate the lower bound as 95% of the maximum price within each year\n",
    "    lower_bound = out_df.groupby(year_col)[price_col].transform(lambda x: np.max(x) * 0.95)\n",
    "    # Filter the DataFrame based on the condition\n",
    "    filtered_df = out_df[out_df[price_col] >= lower_bound]\n",
    "    # Calculate the median for each group of time_year in the filtered DataFrame\n",
    "    median_by_time_year = filtered_df.groupby(year_col)[price_col].median()\n",
    "    # Copy the median values back into the original DataFrame by year_col\n",
    "    out_df.loc[:,'median_price'] = out_df[year_col].map(median_by_time_year)\n",
    "    # Compute the relative discount\n",
    "    pc_disc =  out_df['median_price'] / out_df[price_col]\n",
    "    \n",
    "    \n",
    "    z_scores = (pc_disc - np.mean(pc_disc)) / np.std(pc_disc)\n",
    "    # Identify indices where the absolute z-score is greater than or equal to 3\n",
    "    outlier_indices = np.where(np.abs(z_scores) >= 3)[0]\n",
    "    # Replace outliers with the maximum value from X_pc_d excluding those outliers\n",
    "    if len(outlier_indices) > 0:\n",
    "        pc_disc[outlier_indices] = np.max(pc_disc[~np.isin(np.arange(len(pc_disc)), outlier_indices)])\n",
    "\n",
    "\n",
    "    # Update Price column\n",
    "    out_df.loc[:, price_col] = pc_disc\n",
    "\n",
    "    return out_df , np.mean(pc_disc), np.std(pc_disc)\n",
    "\n",
    "\n",
    "def sales_lag(df, sku = None, neg = True):\n",
    "    out_df = df.copy()\n",
    "    sales_col = f'{sku}_Sales' if sku is not None else 'Sales'\n",
    "    out_df[sales_col] = out_df[sales_col].shift(1) if neg is True else out_df[sales_col].shift(1)\n",
    "    return out_df\n",
    "\n",
    "def sum_columns(df, sku = None, promotype = 'Feature', neg = True):\n",
    "    out_df = df.copy()\n",
    "    columns_to_max = [col for col in out_df.columns if col.startswith(sku+'_'+promotype)] if sku is not None else [col for col in out_df.columns if col.startswith(promotype)]\n",
    "    if not columns_to_max:\n",
    "        # print(f\"No columns found with prefix '{sku}_{type}'\")\n",
    "        return out_df\n",
    "    # Calculate the maximum values using numpy\n",
    "    sum_values = np.sum(out_df[columns_to_max].values, axis=1)\n",
    "    # Create a new column with the maximum values\n",
    "    max_column_name = f'{sku}_{promotype}' if neg is True else f'{promotype}'\n",
    "    out_df[max_column_name] = sum_values if neg is True else sum_values\n",
    "    # Drop the columns used in the max calculation\n",
    "    out_df.drop(columns=columns_to_max, inplace=True)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def demand_coef(data, calendar, store_id_input, sku_id, window_size, train_year_from, train_year_to, alphas):\n",
    "\n",
    "    store_df = data[data['Store_ID'] == store_id_input]\n",
    "    store_sku_df = store_df[store_df['SKU'] == sku_id]\n",
    "\n",
    "\n",
    "    # Get competitor df\n",
    "    store_compet_sku_df = store_df[store_df['SKU'] != sku_id]\n",
    "\n",
    "    # Competitor sku list\n",
    "    compet_sku = store_df[store_df['SKU'] != sku_id]['SKU'].tolist()\n",
    "    compet_sku = list(OrderedDict.fromkeys(compet_sku))\n",
    "\n",
    "    store_compet_sku_df = store_compet_sku_df.pivot_table(index = ['Time_ID', 'Year', 'Store_ID'], columns = 'SKU', \n",
    "                                                        values = ['Price', 'Sales', 'Display1', 'Display2', 'Feature1', 'Feature2', 'Feature3', 'Feature4'])\n",
    "\n",
    "    store_compet_sku_df.columns = ['_'.join([col[1], col[0]]) for col in store_compet_sku_df.columns]\n",
    "    store_compet_sku_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    store_compet_final = store_compet_sku_df.copy()\n",
    "    for sku in compet_sku:\n",
    "        store_compet_final, _, _ = price_discount(store_compet_final, sku)\n",
    "        store_compet_final = sum_columns(store_compet_final, sku, 'Feature')    # Features\n",
    "        store_compet_final = sum_columns(store_compet_final, sku, 'Display')    # Display\n",
    "        store_compet_final = sales_lag(store_compet_final, sku)                 # Lag Sales \n",
    "\n",
    "    drop_cols = ['Store_ID', 'median_price']\n",
    "    store_compet_final.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "    store_sku_df.reset_index(drop = True, inplace= True)\n",
    "\n",
    "    # SKU Features\n",
    "    store_sku_df_part = store_sku_df.copy()\n",
    "    store_sku_df_part, dis_mean, dis_std = price_discount(store_sku_df_part)                                                  # Price Column\n",
    "    store_sku_df_part = sum_columns(store_sku_df_part, promotype = 'Feature', neg = False)                                    # Feature Column\n",
    "    store_sku_df_part = sum_columns(store_sku_df_part, promotype = 'Display', neg = False)                                    # Display Column\n",
    "\n",
    "    store_sku_df_part['Pricelag'] = store_sku_df_part['Price'].shift(1)                                                      # Price Lag Column\n",
    "    store_sku_df_part['Featurelag'] = store_sku_df_part['Feature'].shift(1)                                                  #  Feature Lag Column\n",
    "    store_sku_df_part['Displaylag'] = store_sku_df_part['Display'].shift(1)                                                  #  Display Lag Column\n",
    "\n",
    "    store_sku_df_part['Saleslag'] = store_sku_df_part['Sales'].shift(1)                                                     # Log Sales Lag Column \n",
    "    store_sku_df_part['Sales_mov_avg'] = store_sku_df_part['Sales'].rolling(window = window_size).mean().shift(1)           # Log Sales Rolling Mean Lag Column\n",
    "    store_sku_df_part['Sales'] = store_sku_df_part['Sales']                                                                 # Process Sales for Target Variable\n",
    "\n",
    "\n",
    "    # Add Special Events\n",
    "    event_col = ['Halloween', 'Thanksgiving', 'Christmas', 'NewYear', 'President', 'Easter', 'Memorial', '4thJuly', 'Labour']\n",
    "    event_cols = [col if col == 'NewYear' else [col, f'{col}_1'] for col in event_col]\n",
    "    event_cols = [item for sublist in event_cols for item in ([sublist] if isinstance(sublist, str) else sublist)]\n",
    "    calendar_cols = calendar[['IRI Week']+ event_cols]\n",
    "    calendar_cols = calendar_cols.fillna(0).astype(int)\n",
    "\n",
    "    # Join Special Events to SKU Store Data\n",
    "    store_sku_df_part = pd.merge(store_sku_df_part, calendar_cols, left_on='Time_ID', right_on='IRI Week', how='left')\n",
    "\n",
    "    drop_cols = ['Discount','Store_ID', 'median_price', 'SKU', 'IRI Week']\n",
    "    store_sku_df_part.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "    model_1 = Lasso(max_iter = 10000)\n",
    "    model_2 = Lasso(max_iter = 10000)\n",
    "\n",
    "    store_sku_part_trg = store_sku_df_part[(store_sku_df_part[\"Year\"] >= train_year_from) & (store_sku_df_part[\"Year\"] <= train_year_to)]\n",
    "    store_sku_part_trg = store_sku_part_trg.iloc[window_size:]\n",
    "    # store_sku_part_test = store_sku_df_part[(store_sku_df_part[\"Year\"] == year_test)]\n",
    "\n",
    "    store_compet_trg = store_compet_final[(store_compet_final[\"Year\"] >= train_year_from) & (store_sku_df_part[\"Year\"] <= train_year_to)]\n",
    "    store_compet_trg = store_compet_trg.iloc[window_size:]\n",
    "    # store_compet_test = store_compet_final[(store_compet_final[\"Year\"] == year_test)]\n",
    "\n",
    "    sku_sales_train = store_sku_part_trg['Sales']\n",
    "    # sku_sales_test = store_sku_part_test['Sales']\n",
    "\n",
    "    # Feature Variables\n",
    "    # feature_list = []\n",
    "    sku_train_drop = ['Time_ID', 'Year', 'Sales']\n",
    "    compet_train_drop = ['Time_ID', 'Year']\n",
    "    store_sku_part_trg = store_sku_part_trg.drop(columns = sku_train_drop)\n",
    "    # store_sku_part_test = store_sku_part_test.drop(columns = sku_train_drop)\n",
    "    store_compet_trg = store_compet_trg.drop(columns = compet_train_drop)\n",
    "    # store_compet_test = store_compet_test.drop(columns = compet_train_drop)\n",
    "\n",
    "    # positive_features_1 = ['Price', 'Feature', 'Display', 'Pricelag' ,'Featurelag', 'Displaylag', 'Saleslag']\n",
    "    # model_1.positive = positive_features_1 \n",
    "    model_1.fit(store_sku_part_trg, sku_sales_train)\n",
    "    sku_sales_train_rsd = sku_sales_train - model_1.predict(store_sku_part_trg)\n",
    "\n",
    "    # positive_features_2 = [col for col in store_compet_trg.columns if col.endswith((\"_Display\", \"_Feature\", \"_Price\"))]\n",
    "    # model_2.positive = positive_features_2\n",
    "    model_2.fit(store_compet_trg, sku_sales_train_rsd)\n",
    "\n",
    "    model_1_df = pd.DataFrame(model_1.coef_, index = store_sku_part_trg.columns, columns=[sku_id] )\n",
    "    model_2_df = pd.DataFrame(model_2.coef_, index = store_compet_trg.columns, columns=[sku_id] )\n",
    "\n",
    "    bias_term = model_1.intercept_ + model_2.intercept_\n",
    "\n",
    "    coef_df = pd.concat([model_1_df, model_2_df])\n",
    "\n",
    "    return coef_df, dis_mean, dis_std, bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'To be updated'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m calendar \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(time_dir)\n\u001b[0;32m      4\u001b[0m store_id_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m236117\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'To be updated'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_dir)\n",
    "calendar = pd.read_csv(time_dir)\n",
    "\n",
    "store_id_input = 236117\n",
    "window_size = 8\n",
    "year_from = 2001\n",
    "year_to = 2005\n",
    "year_test = 2006\n",
    "alphas = np.logspace(-4, 0, 100)\n",
    "\n",
    "base_features = ['Price', 'Feature', 'Display', 'Pricelag', 'Featurelag', 'Displaylag', \n",
    "                 'Saleslag', 'Sales_mov_avg', 'Halloween', 'Halloween_1', 'Thanksgiving',\n",
    "                 'Thanksgiving_1', 'Christmas', 'Christmas_1', 'NewYear', 'President',\n",
    "                 'President_1', 'Easter', 'Easter_1', 'Memorial', 'Memorial_1', '4thJuly', '4thJuly_1', 'Labour', 'Labour_1']\n",
    "\n",
    "unique_skus = list(data[data['Store_ID'] == store_id_input]['SKU'].unique())\n",
    "\n",
    "new_index_parts = []\n",
    "for code in unique_skus:\n",
    "    new_index_parts.extend([\n",
    "        f'{code}_Price',\n",
    "        f'{code}_Display',\n",
    "        f'{code}_Feature',\n",
    "        f'{code}_Sales'\n",
    "    ])\n",
    "\n",
    "# Combine the base features with the new index parts\n",
    "complete_index = base_features + new_index_parts\n",
    "\n",
    "# Create an empty DataFrame with the specified list of row indexes\n",
    "final_df = pd.DataFrame(index=complete_index)\n",
    "\n",
    "\n",
    "df_z_score = pd.DataFrame(columns=[\"Mean\", \"Std_deviation\", 'bias'])\n",
    "\n",
    "for sku in tqdm(unique_skus):\n",
    "    df_coef, mean, std, bias = demand_coef(data, calendar, store_id_input, sku, window_size, year_from, year_to, alphas)\n",
    "    final_df = final_df.join(df_coef, how = 'left').fillna(0)\n",
    "    df_z_score.loc[sku] = [mean, std, bias]\n",
    "\n",
    "\n",
    "final_df.index = [idx.replace('Price', 'Discount') for idx in final_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z_score.to_csv('Z_scores.csv')\n",
    "final_df.to_csv('Coefficients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
