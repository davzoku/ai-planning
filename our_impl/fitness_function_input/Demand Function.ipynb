{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_dir = 'combined_milk_final.csv'\n",
    "time_dir = 'time.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emmy/Desktop/Last_term_SMU/AI Planning and decision making/ai-planning/our_impl/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmy/Desktop/Last_term_SMU/AI Planning and decision making/aip_venv/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/emmy/Desktop/Last_term_SMU/AI Planning and decision making/ai-planning/our_impl/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emmy/Desktop/Last_term_SMU/AI Planning and decision making/ai-planning/our_impl/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_discount(df, sku = None, year_col='Year'):\n",
    "    out_df = df.copy()\n",
    "    price_col = f'{sku}_Price' if sku is not None else 'Price'\n",
    "    # Calculate the lower bound as 95% of the maximum price within each year\n",
    "    lower_bound = out_df.groupby(year_col)[price_col].transform(lambda x: np.max(x) * 0.95)\n",
    "    # Filter the DataFrame based on the condition\n",
    "    filtered_df = out_df[out_df[price_col] >= lower_bound]\n",
    "    # Calculate the median for each group of time_year in the filtered DataFrame\n",
    "    median_by_time_year = filtered_df.groupby(year_col)[price_col].median()\n",
    "    # Copy the median values back into the original DataFrame by year_col\n",
    "    out_df.loc[:,'median_price'] = out_df[year_col].map(median_by_time_year)\n",
    "    # Compute the relative discount\n",
    "    pc_disc =  out_df['median_price'] / out_df[price_col]\n",
    "    \n",
    "    \n",
    "    z_scores = (pc_disc - np.mean(pc_disc)) / np.std(pc_disc)\n",
    "    # Identify indices where the absolute z-score is greater than or equal to 3\n",
    "    outlier_indices = np.where(np.abs(z_scores) >= 3)[0]\n",
    "    # Replace outliers with the maximum value from X_pc_d excluding those outliers\n",
    "    if len(outlier_indices) > 0:\n",
    "        pc_disc[outlier_indices] = np.max(pc_disc[~np.isin(np.arange(len(pc_disc)), outlier_indices)])\n",
    "\n",
    "\n",
    "    # Update Price column\n",
    "    out_df.loc[:, price_col] = pc_disc\n",
    "\n",
    "    return out_df , np.mean(pc_disc), np.std(pc_disc)\n",
    "\n",
    "\n",
    "def sales_lag(df, sku = None, neg = True):\n",
    "    out_df = df.copy()\n",
    "    sales_col = f'{sku}_Sales' if sku is not None else 'Sales'\n",
    "    out_df[sales_col] = -np.log(out_df[sales_col].shift(1)) if neg is True else np.log(out_df[sales_col].shift(1))\n",
    "    return out_df\n",
    "\n",
    "def sum_columns(df, sku = None, promotype = 'Feature', neg = True):\n",
    "    out_df = df.copy()\n",
    "    columns_to_max = [col for col in out_df.columns if col.startswith(sku+'_'+promotype)] if sku is not None else [col for col in out_df.columns if col.startswith(promotype)]\n",
    "    if not columns_to_max:\n",
    "        # print(f\"No columns found with prefix '{sku}_{type}'\")\n",
    "        return out_df\n",
    "    # Calculate the maximum values using numpy\n",
    "    sum_values = np.sum(out_df[columns_to_max].values, axis=1)\n",
    "    # Create a new column with the maximum values\n",
    "    max_column_name = f'{sku}_{promotype}' if neg is True else f'{promotype}'\n",
    "    out_df[max_column_name] = -sum_values if neg is True else sum_values\n",
    "    # Drop the columns used in the max calculation\n",
    "    out_df.drop(columns=columns_to_max, inplace=True)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def demand_coef(data, calendar, store_id_input, sku_id, window_size, train_year_from, train_year_to, alphas):\n",
    "\n",
    "    store_df = data[data['Store_ID'] == store_id_input]\n",
    "    store_sku_df = store_df[store_df['SKU'] == sku_id]\n",
    "\n",
    "\n",
    "    # Get competitor df\n",
    "    store_compet_sku_df = store_df[store_df['SKU'] != sku_id]\n",
    "\n",
    "    # Competitor sku list\n",
    "    compet_sku = store_df[store_df['SKU'] != sku_id]['SKU'].tolist()\n",
    "    compet_sku = list(OrderedDict.fromkeys(compet_sku))\n",
    "\n",
    "    store_compet_sku_df = store_compet_sku_df.pivot_table(index = ['Time_ID', 'Year', 'Store_ID'], columns = 'SKU', \n",
    "                                                        values = ['Price', 'Sales', 'Display1', 'Display2', 'Feature1', 'Feature2', 'Feature3', 'Feature4'])\n",
    "\n",
    "    store_compet_sku_df.columns = ['_'.join([col[1], col[0]]) for col in store_compet_sku_df.columns]\n",
    "    store_compet_sku_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    store_compet_final = store_compet_sku_df.copy()\n",
    "    for sku in compet_sku:\n",
    "        store_compet_final, _, _ = price_discount(store_compet_final, sku)\n",
    "        store_compet_final = sum_columns(store_compet_final, sku, 'Feature')    # Negative Features\n",
    "        store_compet_final = sum_columns(store_compet_final, sku, 'Display')    # Negative Display\n",
    "        store_compet_final = sales_lag(store_compet_final, sku)                 # Negative Lag Sales \n",
    "\n",
    "    drop_cols = ['Store_ID', 'median_price']\n",
    "    store_compet_final.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "    store_sku_df.reset_index(drop = True, inplace= True)\n",
    "\n",
    "    # SKU Features\n",
    "    store_sku_df_part = store_sku_df.copy()\n",
    "    store_sku_df_part, dis_mean, dis_std = price_discount(store_sku_df_part)                                                        # Price Column\n",
    "    store_sku_df_part = sum_columns(store_sku_df_part, promotype = 'Feature', neg = False)                                          # Feature Column\n",
    "    store_sku_df_part = sum_columns(store_sku_df_part, promotype = 'Display', neg = False)                                          # Display Column\n",
    "\n",
    "    store_sku_df_part['Pricelag'] = -store_sku_df_part['Price'].shift(1)                                                      # Negative Price Lag Column\n",
    "    store_sku_df_part['Featurelag'] = -store_sku_df_part['Feature'].shift(1)                                                        # Negative Feature Lag Column\n",
    "    store_sku_df_part['Displaylag'] = -store_sku_df_part['Display'].shift(1)                                                        # Negative Display Lag Column\n",
    "\n",
    "    store_sku_df_part['Saleslag'] = -np.log(store_sku_df_part['Sales'].shift(1))                                                    # Negative Log Sales Lag Column \n",
    "    store_sku_df_part['Sales_mov_avg'] = np.log(store_sku_df_part['Sales'].rolling(window = window_size).mean()).shift(1)           # Log Sales Rolling Mean Lag Column\n",
    "    store_sku_df_part['Sales'] = np.log(store_sku_df_part['Sales'])                                                                 # Process Sales for Target Variable\n",
    "\n",
    "\n",
    "    # Add Special Events\n",
    "    event_col = ['Halloween', 'Thanksgiving', 'Christmas', 'NewYear', 'President', 'Easter', 'Memorial', '4thJuly', 'Labour']\n",
    "    event_cols = [col if col == 'NewYear' else [col, f'{col}_1'] for col in event_col]\n",
    "    event_cols = [item for sublist in event_cols for item in ([sublist] if isinstance(sublist, str) else sublist)]\n",
    "    calendar_cols = calendar[['IRI Week']+ event_cols]\n",
    "    calendar_cols = calendar_cols.fillna(0).astype(int)\n",
    "\n",
    "    # Join Special Events to SKU Store Data\n",
    "    store_sku_df_part = pd.merge(store_sku_df_part, calendar_cols, left_on='Time_ID', right_on='IRI Week', how='left')\n",
    "\n",
    "    drop_cols = ['Discount','Store_ID', 'median_price', 'SKU', 'IRI Week']\n",
    "    store_sku_df_part.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "    model_1 = LassoCV(cv = 3, alphas= alphas, max_iter = 1000)\n",
    "    model_2 = LassoCV(cv = 3, alphas= alphas, max_iter = 1000)\n",
    "\n",
    "    store_sku_part_trg = store_sku_df_part[(store_sku_df_part[\"Year\"] >= train_year_from) & (store_sku_df_part[\"Year\"] <= train_year_to)]\n",
    "    store_sku_part_trg = store_sku_part_trg.iloc[window_size:]\n",
    "    # store_sku_part_test = store_sku_df_part[(store_sku_df_part[\"Year\"] == year_test)]\n",
    "\n",
    "    store_compet_trg = store_compet_final[(store_compet_final[\"Year\"] >= train_year_from) & (store_sku_df_part[\"Year\"] <= train_year_to)]\n",
    "    store_compet_trg = store_compet_trg.iloc[window_size:]\n",
    "    # store_compet_test = store_compet_final[(store_compet_final[\"Year\"] == year_test)]\n",
    "\n",
    "    sku_sales_train = store_sku_part_trg['Sales']\n",
    "    # sku_sales_test = store_sku_part_test['Sales']\n",
    "\n",
    "    # Feature Variables\n",
    "    # feature_list = []\n",
    "    sku_train_drop = ['Time_ID', 'Year', 'Sales']\n",
    "    compet_train_drop = ['Time_ID', 'Year']\n",
    "    store_sku_part_trg = store_sku_part_trg.drop(columns = sku_train_drop)\n",
    "    # store_sku_part_test = store_sku_part_test.drop(columns = sku_train_drop)\n",
    "    store_compet_trg = store_compet_trg.drop(columns = compet_train_drop)\n",
    "    # store_compet_test = store_compet_test.drop(columns = compet_train_drop)\n",
    "\n",
    "    positive_features_1 = ['Price', 'Feature', 'Display', 'Pricelag' ,'Featurelag', 'Displaylag', 'Saleslag']\n",
    "    model_1.positive = positive_features_1 \n",
    "    model_1.fit(store_sku_part_trg, sku_sales_train)\n",
    "    sku_sales_train_rsd = sku_sales_train - model_1.predict(store_sku_part_trg)\n",
    "\n",
    "    positive_features_2 = [col for col in store_compet_trg.columns if col.endswith((\"_Display\", \"_Feature\", \"_Price\"))]\n",
    "    model_2.positive = positive_features_2\n",
    "    model_2.fit(store_compet_trg, sku_sales_train_rsd)\n",
    "\n",
    "    model_1_df = pd.DataFrame(model_1.coef_, index = store_sku_part_trg.columns, columns=[sku_id] )\n",
    "    model_2_df = pd.DataFrame(model_2.coef_, index = store_compet_trg.columns, columns=[sku_id] )\n",
    "\n",
    "    coef_df = pd.concat([model_1_df, model_2_df])\n",
    "\n",
    "    return coef_df, dis_mean, dis_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'positive' parameter of LassoCV must be an instance of 'bool' or an instance of 'numpy.bool_'. Got ['Price', 'Feature', 'Display', 'Pricelag', 'Featurelag', 'Displaylag', 'Saleslag'] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m df_z_score \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStd_deviation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sku \u001b[38;5;129;01min\u001b[39;00m tqdm(unique_skus):\n\u001b[0;32m---> 37\u001b[0m     df_coef, mean, std \u001b[38;5;241m=\u001b[39m \u001b[43mdemand_coef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalendar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_id_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msku\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mjoin(df_coef, how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m     df_z_score\u001b[38;5;241m.\u001b[39mloc[sku] \u001b[38;5;241m=\u001b[39m [mean, std]\n",
      "Cell \u001b[0;32mIn[4], line 137\u001b[0m, in \u001b[0;36mdemand_coef\u001b[0;34m(data, calendar, store_id_input, sku_id, window_size, train_year_from, train_year_to, alphas)\u001b[0m\n\u001b[1;32m    135\u001b[0m positive_features_1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisplay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPricelag\u001b[39m\u001b[38;5;124m'\u001b[39m ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeaturelag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisplaylag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaleslag\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    136\u001b[0m model_1\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;241m=\u001b[39m positive_features_1 \n\u001b[0;32m--> 137\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_sku_part_trg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msku_sales_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m sku_sales_train_rsd \u001b[38;5;241m=\u001b[39m sku_sales_train \u001b[38;5;241m-\u001b[39m model_1\u001b[38;5;241m.\u001b[39mpredict(store_sku_part_trg)\n\u001b[1;32m    140\u001b[0m positive_features_2 \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m store_compet_trg\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Display\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Feature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Price\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n",
      "File \u001b[0;32m~/Desktop/Last_term_SMU/AI Planning and decision making/aip_venv/lib/python3.9/site-packages/sklearn/base.py:1467\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1463\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1464\u001b[0m )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Last_term_SMU/AI Planning and decision making/aip_venv/lib/python3.9/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Last_term_SMU/AI Planning and decision making/aip_venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'positive' parameter of LassoCV must be an instance of 'bool' or an instance of 'numpy.bool_'. Got ['Price', 'Feature', 'Display', 'Pricelag', 'Featurelag', 'Displaylag', 'Saleslag'] instead."
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_dir)\n",
    "calendar = pd.read_csv(time_dir)\n",
    "\n",
    "store_id_input = 236117\n",
    "window_size = 8\n",
    "year_from = 2001\n",
    "year_to = 2005\n",
    "year_test = 2006\n",
    "alphas = np.logspace(-4, 0, 100)\n",
    "\n",
    "base_features = ['Price', 'Feature', 'Display', 'Pricelag', 'Featurelag', 'Displaylag', \n",
    "                 'Saleslag', 'Sales_mov_avg', 'Halloween', 'Halloween_1', 'Thanksgiving',\n",
    "                 'Thanksgiving_1', 'Christmas', 'Christmas_1', 'NewYear', 'President',\n",
    "                 'President_1', 'Easter', 'Easter_1', 'Memorial', 'Memorial_1', '4thJuly', '4thJuly_1', 'Labour', 'Labour_1']\n",
    "\n",
    "unique_skus = list(data[data['Store_ID'] == store_id_input]['SKU'].unique())\n",
    "\n",
    "new_index_parts = []\n",
    "for code in unique_skus:\n",
    "    new_index_parts.extend([\n",
    "        f'{code}_Price',\n",
    "        f'{code}_Display',\n",
    "        f'{code}_Feature',\n",
    "        f'{code}_Sales'\n",
    "    ])\n",
    "\n",
    "# Combine the base features with the new index parts\n",
    "complete_index = base_features + new_index_parts\n",
    "\n",
    "# Create an empty DataFrame with the specified list of row indexes\n",
    "final_df = pd.DataFrame(index=complete_index)\n",
    "\n",
    "\n",
    "df_z_score = pd.DataFrame(columns=[\"Mean\", \"Std_deviation\"])\n",
    "\n",
    "for sku in tqdm(unique_skus):\n",
    "    df_coef, mean, std = demand_coef(data, calendar, store_id_input, sku, window_size, year_from, year_to, alphas)\n",
    "    final_df = final_df.join(df_coef, how = 'left').fillna(0)\n",
    "    df_z_score.loc[sku] = [mean, std]\n",
    "\n",
    "\n",
    "final_df.index = [idx.replace('Price', 'Discount') for idx in final_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z_score.to_csv('Z_scores.csv')\n",
    "final_df.to_csv('Coefficients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
