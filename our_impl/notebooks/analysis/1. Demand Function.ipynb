{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_dir = '../assets/combined_milk_final.csv'\n",
    "time_dir = '../assets/time.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_discount(df, sku = None, year_col='Year'):\n",
    "    out_df = df.copy()\n",
    "    price_col = f'{sku}_Price' if sku is not None else 'Price'\n",
    "    # Calculate the lower bound as 95% of the maximum price within each year\n",
    "    lower_bound = out_df.groupby(year_col)[price_col].transform(lambda x: np.max(x) * 0.95)\n",
    "    # Filter the DataFrame based on the condition\n",
    "    filtered_df = out_df[out_df[price_col] >= lower_bound]\n",
    "    # Calculate the median for each group of time_year in the filtered DataFrame\n",
    "    median_by_time_year = filtered_df.groupby(year_col)[price_col].median()\n",
    "    # Copy the median values back into the original DataFrame by year_col\n",
    "    out_df.loc[:,'median_price'] = out_df[year_col].map(median_by_time_year)\n",
    "    # Compute the relative discount\n",
    "    pc_disc =  out_df['median_price'] / out_df[price_col]\n",
    "    \n",
    "    \n",
    "    z_scores = (pc_disc - np.mean(pc_disc)) / np.std(pc_disc)\n",
    "    # Identify indices where the absolute z-score is greater than or equal to 3\n",
    "    outlier_indices = np.where(np.abs(z_scores) >= 3)[0]\n",
    "    # Replace outliers with the maximum value from X_pc_d excluding those outliers\n",
    "    if len(outlier_indices) > 0:\n",
    "        pc_disc[outlier_indices] = np.max(pc_disc[~np.isin(np.arange(len(pc_disc)), outlier_indices)])\n",
    "\n",
    "\n",
    "    # Update Price column\n",
    "    out_df.loc[:, price_col] = pc_disc\n",
    "\n",
    "    return out_df , np.mean(pc_disc), np.std(pc_disc)\n",
    "\n",
    "\n",
    "def sales_lag(df, sku = None, neg = True):\n",
    "    out_df = df.copy()\n",
    "    sales_col = f'{sku}_Sales' if sku is not None else 'Sales'\n",
    "    out_df[sales_col] = out_df[sales_col].shift(1) if neg is True else out_df[sales_col].shift(1)\n",
    "    return out_df\n",
    "\n",
    "def sum_columns(df, sku = None, promotype = 'Feature', neg = True):\n",
    "    out_df = df.copy()\n",
    "    columns_to_max = [col for col in out_df.columns if col.startswith(sku+'_'+promotype)] if sku is not None else [col for col in out_df.columns if col.startswith(promotype)]\n",
    "    if not columns_to_max:\n",
    "        return out_df\n",
    "    # Calculate the maximum values using numpy\n",
    "    sum_values = np.sum(out_df[columns_to_max].values, axis=1)\n",
    "    # Create a new column with the maximum values\n",
    "    max_column_name = f'{sku}_{promotype}' if neg is True else f'{promotype}'\n",
    "    out_df[max_column_name] = sum_values if neg is True else sum_values\n",
    "    # Drop the columns used in the max calculation\n",
    "    out_df.drop(columns=columns_to_max, inplace=True)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def demand_coef(data, calendar, store_id_input, sku_id, window_size, train_year_from, train_year_to, test_year):\n",
    "\n",
    "    store_df = data[data['Store_ID'] == store_id_input]\n",
    "    store_sku_df = store_df[store_df['SKU'] == sku_id]\n",
    "\n",
    "\n",
    "    # Get competitor df\n",
    "    store_compet_sku_df = store_df[store_df['SKU'] != sku_id]\n",
    "\n",
    "    # Competitor sku list\n",
    "    compet_sku = store_df[store_df['SKU'] != sku_id]['SKU'].tolist()\n",
    "    compet_sku = list(OrderedDict.fromkeys(compet_sku))\n",
    "\n",
    "    store_compet_sku_df = store_compet_sku_df.pivot_table(index = ['Time_ID', 'Year', 'Store_ID'], columns = 'SKU', \n",
    "                                                        values = ['Price', 'Sales', 'Display1', 'Display2', 'Feature1', 'Feature2', 'Feature3', 'Feature4'])\n",
    "\n",
    "    store_compet_sku_df.columns = ['_'.join([col[1], col[0]]) for col in store_compet_sku_df.columns]\n",
    "    store_compet_sku_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    store_compet_final = store_compet_sku_df.copy()\n",
    "    for sku in compet_sku:\n",
    "        store_compet_final, _, _ = price_discount(store_compet_final, sku)\n",
    "        store_compet_final = sum_columns(store_compet_final, sku, 'Feature')    # Features\n",
    "        store_compet_final = sum_columns(store_compet_final, sku, 'Display')    # Display\n",
    "        store_compet_final = sales_lag(store_compet_final, sku)                 # Lag Sales \n",
    "\n",
    "    drop_cols = ['Store_ID', 'median_price']\n",
    "    store_compet_final.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "    store_sku_df.reset_index(drop = True, inplace= True)\n",
    "\n",
    "    # SKU Features\n",
    "    store_sku_df_part = store_sku_df.copy()\n",
    "    store_sku_df_part, dis_mean, dis_std = price_discount(store_sku_df_part)                                                  # Price Column\n",
    "    store_sku_df_part = sum_columns(store_sku_df_part, promotype = 'Feature', neg = False)                                    # Feature Column\n",
    "    store_sku_df_part = sum_columns(store_sku_df_part, promotype = 'Display', neg = False)                                    # Display Column\n",
    "\n",
    "    store_sku_df_part['Pricelag'] = store_sku_df_part['Price'].shift(1)                                                      # Price Lag Column\n",
    "    store_sku_df_part['Featurelag'] = store_sku_df_part['Feature'].shift(1)                                                  #  Feature Lag Column\n",
    "    store_sku_df_part['Displaylag'] = store_sku_df_part['Display'].shift(1)                                                  #  Display Lag Column\n",
    "\n",
    "    store_sku_df_part['Saleslag'] = store_sku_df_part['Sales'].shift(1)                                                     # Log Sales Lag Column \n",
    "    store_sku_df_part['Sales_mov_avg'] = store_sku_df_part['Sales'].rolling(window = window_size).mean().shift(1)           # Log Sales Rolling Mean Lag Column\n",
    "    store_sku_df_part['Sales'] = store_sku_df_part['Sales']                                                                 # Process Sales for Target Variable\n",
    "\n",
    "\n",
    "    # Add Special Events\n",
    "    event_col = ['Halloween', 'Thanksgiving', 'Christmas', 'NewYear', 'President', 'Easter', 'Memorial', '4thJuly', 'Labour']\n",
    "    event_cols = [col if col == 'NewYear' else [col, f'{col}_1'] for col in event_col]\n",
    "    event_cols = [item for sublist in event_cols for item in ([sublist] if isinstance(sublist, str) else sublist)]\n",
    "    calendar_cols = calendar[['IRI Week']+ event_cols]\n",
    "    calendar_cols = calendar_cols.fillna(0).astype(int)\n",
    "\n",
    "    # Join Special Events to SKU Store Data\n",
    "    store_sku_df_part = pd.merge(store_sku_df_part, calendar_cols, left_on='Time_ID', right_on='IRI Week', how='left')\n",
    "\n",
    "    drop_cols = ['Discount','Store_ID', 'median_price', 'SKU', 'IRI Week']\n",
    "    store_sku_df_part.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "    # Train Models\n",
    "    model_1 = Lasso(max_iter = 10000)\n",
    "    model_2 = Lasso(max_iter = 10000)\n",
    "\n",
    "    # Prepare training data for own + event features\n",
    "    store_sku_part_trg = store_sku_df_part[(store_sku_df_part[\"Year\"] >= train_year_from) & (store_sku_df_part[\"Year\"] <= train_year_to)]\n",
    "    store_sku_part_trg = store_sku_part_trg.iloc[window_size:]\n",
    "    \n",
    "    # Prepare training data for competitor sku features\n",
    "    store_compet_trg = store_compet_final[(store_compet_final[\"Year\"] >= train_year_from) & (store_sku_df_part[\"Year\"] <= train_year_to)]\n",
    "    store_compet_trg = store_compet_trg.iloc[window_size:]\n",
    "\n",
    "    # Training Target variable\n",
    "    sku_sales_train = store_sku_part_trg['Sales']\n",
    "\n",
    "    # Feature Variables\n",
    "    sku_train_drop = ['Time_ID', 'Year', 'Sales']\n",
    "    compet_train_drop = ['Time_ID', 'Year']\n",
    "    store_sku_part_trg = store_sku_part_trg.drop(columns = sku_train_drop)\n",
    "    store_compet_trg = store_compet_trg.drop(columns = compet_train_drop)\n",
    "\n",
    "    # Fit model 1\n",
    "    model_1.fit(store_sku_part_trg, sku_sales_train)\n",
    "    sku_sales_train_rsd = sku_sales_train - model_1.predict(store_sku_part_trg)\n",
    "    # Fit model 2\n",
    "    model_2.fit(store_compet_trg, sku_sales_train_rsd)\n",
    "\n",
    "    # Save coefficients\n",
    "    model_1_df = pd.DataFrame(model_1.coef_, index = store_sku_part_trg.columns, columns=[sku_id] )\n",
    "    model_2_df = pd.DataFrame(model_2.coef_, index = store_compet_trg.columns, columns=[sku_id] )\n",
    "\n",
    "    # Get bias term of both models\n",
    "    bias_term = model_1.intercept_ + model_2.intercept_\n",
    "\n",
    "\n",
    "    coef_df = pd.concat([model_1_df, model_2_df])\n",
    "\n",
    "    # Prepare test data\n",
    "    store_sku_part_test = store_sku_df_part[(store_sku_df_part[\"Year\"] == test_year)]\n",
    "    store_compet_test = store_compet_final[(store_compet_final[\"Year\"] == test_year)]\n",
    "\n",
    "    # Test Target variabe\n",
    "    sku_sales_test = store_sku_part_test['Sales']\n",
    "    store_sku_part_test = store_sku_part_test.drop(columns = sku_train_drop)\n",
    "    store_compet_test = store_compet_test.drop(columns = compet_train_drop)\n",
    "\n",
    "    full_predict = model_1.predict(store_sku_part_test) + model_2.predict(store_compet_test)\n",
    "    full_predict = np.maximum(0, full_predict)\n",
    "\n",
    "    mse =  mean_squared_error(sku_sales_test, full_predict)\n",
    "    mape = mean_absolute_percentage_error(sku_sales_test, full_predict)\n",
    "    mae = mean_absolute_error(sku_sales_test, full_predict)\n",
    "\n",
    "    \n",
    "    return coef_df, dis_mean, dis_std, bias_term, mse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:19<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_dir)\n",
    "calendar = pd.read_csv(time_dir)\n",
    "\n",
    "store_id_input = 236117\n",
    "window_size = 8\n",
    "year_from = 2001\n",
    "year_to = 2005\n",
    "year_test = 2006\n",
    "alphas = np.logspace(-4, 0, 100)\n",
    "\n",
    "base_features = ['Price', 'Feature', 'Display', 'Pricelag', 'Featurelag', 'Displaylag', \n",
    "                 'Saleslag', 'Sales_mov_avg', 'Halloween', 'Halloween_1', 'Thanksgiving',\n",
    "                 'Thanksgiving_1', 'Christmas', 'Christmas_1', 'NewYear', 'President',\n",
    "                 'President_1', 'Easter', 'Easter_1', 'Memorial', 'Memorial_1', '4thJuly', '4thJuly_1', 'Labour', 'Labour_1']\n",
    "\n",
    "unique_skus = list(data[data['Store_ID'] == store_id_input]['SKU'].unique())\n",
    "\n",
    "new_index_parts = []\n",
    "for code in unique_skus:\n",
    "    new_index_parts.extend([\n",
    "        f'{code}_Price',\n",
    "        f'{code}_Display',\n",
    "        f'{code}_Feature',\n",
    "        f'{code}_Sales'\n",
    "    ])\n",
    "\n",
    "# Combine the base features with the new index parts\n",
    "complete_index = base_features + new_index_parts\n",
    "\n",
    "# Create an empty DataFrame with the specified list of row indexes\n",
    "final_df = pd.DataFrame(index=complete_index)\n",
    "\n",
    "\n",
    "df_z_score = pd.DataFrame(columns=[\"Mean\", \"Std_deviation\", 'bias'])\n",
    "df_metrics = pd.DataFrame(columns=['MSE', \"MAE\", \"MAPE\"])\n",
    "\n",
    "for sku in tqdm(unique_skus):\n",
    "    df_coef, mean, std, bias, mse, mae, mape = demand_coef(data, calendar, store_id_input, sku, window_size, year_from, year_to, year_test)\n",
    "    final_df = final_df.join(df_coef, how = 'left').fillna(0)\n",
    "    df_z_score.loc[sku] = [mean, std, bias]\n",
    "    df_metrics.loc[sku] = [mse, mae, mape]\n",
    "\n",
    "\n",
    "final_df.index = [idx.replace('Price', 'Discount') for idx in final_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "df_z_score_out = df_z_score.reset_index()\n",
    "df_z_score_out = df_z_score_out.rename(columns={'index': 'SKU'})\n",
    "df_z_score_out.to_csv('../assets/Z_scores.csv', index = False)\n",
    "final_df.to_csv('../assets/Coefficients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_csv('../assets/Demand_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32467696837545384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics['MAPE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
